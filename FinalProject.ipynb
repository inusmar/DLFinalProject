{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FinalProject.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AmAC4Uuo9nxD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592737281463,"user_tz":-120,"elapsed":2839,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"ef77bf37-20f5-4fc6-97ca-2f0d70807acf"},"source":["# Import the dependencies\n","import numpy as np\n","import pandas as pd\n","import sys \n","from keras.models import Sequential\n","from keras.layers import LSTM, Activation, Flatten, Dropout, Dense, Embedding, TimeDistributed, CuDNNLSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n","from google.colab import drive\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"240P1v6sitxK","colab_type":"text"},"source":["We load the song dataset and filter by the name of the song and lyrics.\n"]},{"cell_type":"code","metadata":{"id":"HW2ULtMg9tJA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1592737307475,"user_tz":-120,"elapsed":24130,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"77f19a92-668c-4721-97ce-216b13eb5c91"},"source":["drive.mount('/content/drive')\n","data_path = '/content/drive/My Drive/DeepLearning_2020/test/songdata.csv'\n","\n","dataset = pd.read_csv(data_path)\n","print(\"Our dataset contains \"+str(dataset.shape[0])+\" song lyrics.\")\n","#print(dataset.columns)\n","print(\"of \"+str(len(dataset['artist'].unique()))+ \" different artists\")\n","datas = dataset.filter(['song','text'],axis=1)\n","#for col in datas.columns:\n","#  print(col)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","Our dataset contains 57650 song lyrics.\n","of 643 different artists\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EOSeZLQp9vXc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1592737317713,"user_tz":-120,"elapsed":1208,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"80c5afdf-a033-47bc-f67b-886025267ce5"},"source":["datas.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>song</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ahe's My Kind Of Girl</td>\n","      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Andante, Andante</td>\n","      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>As Good As New</td>\n","      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Bang</td>\n","      <td>Making somebody happy is a question of give an...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Bang-A-Boomerang</td>\n","      <td>Making somebody happy is a question of give an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                    song                                               text\n","0  Ahe's My Kind Of Girl  Look at her face, it's a wonderful face  \\nAnd...\n","1       Andante, Andante  Take it easy with me, please  \\nTouch me gentl...\n","2         As Good As New  I'll never know why I had to go  \\nWhy I had t...\n","3                   Bang  Making somebody happy is a question of give an...\n","4       Bang-A-Boomerang  Making somebody happy is a question of give an..."]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"ztPga5PrkFgr","colab_type":"text"},"source":["To have all the lyrics as a training set, we will write in a txt file all the lyrics of all songs. This will be useful for training data in the LSTM model."]},{"cell_type":"code","metadata":{"id":"7CDAIEbV9z09","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1592737325489,"user_tz":-120,"elapsed":1925,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"77fa7523-17cd-4de1-ab6d-ecf8efab6068"},"source":["lyrics_data = pd.DataFrame({'songID': dataset.index, 'songName':dataset['song'], 'lyrics':dataset['text'] })\n","raw_text = ','.join(lyrics_data['lyrics'])\n","raw_text = raw_text.lower()\n","lyrics_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>songID</th>\n","      <th>songName</th>\n","      <th>lyrics</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Ahe's My Kind Of Girl</td>\n","      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Andante, Andante</td>\n","      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>As Good As New</td>\n","      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Bang</td>\n","      <td>Making somebody happy is a question of give an...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Bang-A-Boomerang</td>\n","      <td>Making somebody happy is a question of give an...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   songID  ...                                             lyrics\n","0       0  ...  Look at her face, it's a wonderful face  \\nAnd...\n","1       1  ...  Take it easy with me, please  \\nTouch me gentl...\n","2       2  ...  I'll never know why I had to go  \\nWhy I had t...\n","3       3  ...  Making somebody happy is a question of give an...\n","4       4  ...  Making somebody happy is a question of give an...\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"039i81XHmWTg","colab_type":"text"},"source":["As we will perform a char-level RNN and LSTM, we are going to store all unique characters and get the size of the vocabulary we are using. We will also map each character to an index."]},{"cell_type":"code","metadata":{"id":"PG0TB-aMmaiQ","colab_type":"code","colab":{}},"source":["chars = sorted(list(set(raw_text)))\n","vocabulary_size = len(chars)\n","\n","char_to_ix = {ch: i for i, ch in enumerate(chars)}\n","ix_to_char = {i: ch for i, ch in enumerate(chars)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_FWuW16j_UO","colab_type":"text"},"source":["RNN model"]},{"cell_type":"code","metadata":{"id":"TvIs2zLMkQ3w","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import random\n","tf.compat.v1.disable_eager_execution()\n","data = raw_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uM4iPmvXmyDh","colab_type":"text"},"source":["Given an index of a character, returns the one-hot encoded vectors."]},{"cell_type":"code","metadata":{"id":"YzcybCj0mBv9","colab_type":"code","colab":{}},"source":["def one_hot_encoder(index):\n","    return np.eye(vocabulary_size)[index]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B67198Dbm6U9","colab_type":"text"},"source":["Now we define the RNN network parameters."]},{"cell_type":"code","metadata":{"id":"vX-Up6Jmm-ao","colab_type":"code","colab":{}},"source":["#units in the hidden layer\n","hidden_size = 120\n","#length output\n","seq_length = 200  \n","#define learning rate for gradient descent is as follows:\n","learning_rate = 0.0000000001\n","#set the seed value:\n","seed_value = 42\n","tf.compat.v1.set_random_seed(seed_value)\n","random.seed(seed_value)\n","\n","inputs =tf.compat.v1.placeholder(shape=[None, vocabulary_size],dtype=tf.float32, name=\"inputs\")\n","targets = tf.compat.v1.placeholder(shape=[None, vocabulary_size], dtype=tf.float32, name=\"targets\")\n","init_state = tf.compat.v1.placeholder(shape=[1, hidden_size], dtype=tf.float32, name=\"state\")\n","initializer = tf.compat.v1.random_normal_initializer(stddev=0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i-IepTOVnVdZ","colab_type":"text"},"source":["Forward propagation:\n"]},{"cell_type":"code","metadata":{"id":"lJAQ9oKAoJ83","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1592737340728,"user_tz":-120,"elapsed":1498,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"1069fe6a-4127-46f9-dd13-c9bee8f195b2"},"source":["with tf.compat.v1.variable_scope(\"RNN\") as scope:\n","  h_t = init_state\n","  y_hat = []\n","\n","  for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n","      if t > 0:\n","          scope.reuse_variables()  \n","\n","      #input to hidden layer weights\n","      U = tf.compat.v1.get_variable(\"U\", [vocabulary_size, hidden_size], initializer=initializer)\n","      #hidden to hidden layer weights\n","      W = tf.compat.v1.get_variable(\"W\", [hidden_size, hidden_size], initializer=initializer)\n","      #output to hidden layer weights\n","      V = tf.compat.v1.get_variable(\"V\", [hidden_size, vocabulary_size], initializer=initializer)\n","      #bias for hidden layer\n","      bh = tf.compat.v1.get_variable(\"bh\", [hidden_size], initializer=initializer)\n","      #bias for output layer\n","      by = tf.compat.v1.get_variable(\"by\", [vocabulary_size], initializer=initializer)\n","      h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n","      y_hat_t = tf.matmul(h_t, V) + by\n","      y_hat.append(y_hat_t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yaMSS7RGoU2p","colab_type":"text"},"source":["Now we apply the activation function (softmax)."]},{"cell_type":"code","metadata":{"id":"6Bj5h9buoaFM","colab_type":"code","colab":{}},"source":["output_softmax = tf.nn.softmax(y_hat[-1])  \n","outputs = tf.concat(y_hat, axis=0)\n","#compute the loss and store the final hidden state in hprev --> we will use it for the predictions\n","loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs))\n","hprev = h_t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w3LkNMAQo3Oa","colab_type":"text"},"source":["We need to compute the gradients of the loss using the Adam optimizer."]},{"cell_type":"code","metadata":{"id":"Magno1B-o9vf","colab_type":"code","colab":{}},"source":["minimizer = tf.compat.v1.train.AdamOptimizer()\n","gradients = minimizer.compute_gradients(loss)\n","#there are gradients that exceed the threshold --> clipped!\n","threshold = tf.constant(5.0, name=\"grad_clipping\")\n","clipped_gradients = []\n","for grad, var in gradients:\n","    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n","    clipped_gradients.append((clipped_grad, var))\n","\n","updated_gradients = minimizer.apply_gradients(clipped_gradients)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QiOOJmLUpIKI","colab_type":"text"},"source":["We start running the model with the tensorflow session."]},{"cell_type":"code","metadata":{"id":"vlpImBTapQ34","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":799},"executionInfo":{"status":"error","timestamp":1592737600350,"user_tz":-120,"elapsed":11078,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"356ea83e-a925-4c52-8ff8-4daba8cc8053"},"source":["sess = tf.compat.v1.Session()\n","init = tf.compat.v1.global_variables_initializer()\n","sess.run(init)\n","#points the character in our dataset\n","pointer = 0\n","iteration = 0\n","maxiters = 20000\n","while iteration < maxiters:\n","    if pointer + seq_length+1 >= len(data) or iteration == 0:\n","        hprev_val = np.zeros([1, hidden_size])\n","        pointer = 0  \n","    \n","    #select input sentence --> slice the data\n","    input_sentence = data[pointer:pointer + seq_length]\n","    \n","    #select output sentence\n","    output_sentence = data[pointer + 1:pointer + seq_length + 1]\n","    \n","    #get the indices of input and output sentence\n","    input_indices = [char_to_ix[ch] for ch in input_sentence]\n","    target_indices = [char_to_ix[ch] for ch in output_sentence]\n","\n","    #convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n","    input_vector = one_hot_encoder(input_indices)\n","    target_vector = one_hot_encoder(target_indices)\n","\n","    \n","    #train the network and get the final hidden state\n","    hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n","                                      feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n","   \n","       \n","    #make predictions on every 500th iteration \n","    if iteration % 500 == 0:\n","\n","        #length of characters we want to predict\n","        sample_length = 500\n","        \n","        #randomly select index\n","        random_index = random.randint(0, len(data) - seq_length)\n","        \n","        #sample the input sentence with the randomly selected index\n","        sample_input_sent = data[random_index:random_index + seq_length]\n","    \n","        #get the indices of the sampled input sentence\n","        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n","        \n","        #store the final hidden state in sample_prev_state_val\n","        sample_prev_state_val = np.copy(hprev_val)\n","        \n","        #for storing the indices of predicted characters\n","        predicted_indices = []\n","        \n","        \n","        for t in range(sample_length):\n","            \n","            #convert the sampled input sentence into one-hot encoded vector using their indices\n","            sample_input_vector = one_hot_encoder(sample_input_indices)\n","            \n","            #compute the probability of all the words in the vocabulary to be the next character\n","            probs_dist, sample_prev_state_val = sess.run([output_softmax, hprev],\n","                                                      feed_dict={inputs: sample_input_vector,init_state: sample_prev_state_val})\n","\n","            #we randomly select the index with the probabilty distribtuion generated by the model\n","            ix = np.random.choice(range(vocabulary_size), p=probs_dist.ravel())\n","            \n","            sample_input_indices = sample_input_indices[1:] + [ix]\n","            \n","            \n","            #store the predicted index in predicted_indices list\n","            predicted_indices.append(ix)\n","            \n","        #convert the predicted indices to their character\n","        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n","        \n","        #combine the predcited characters\n","        text = ''.join(predicted_chars)\n","        \n","        #predict the predict text on every 50000th iteration\n","        if iteration %1000 == 0:           \n","            print ('\\n')\n","            print (' After %d iterations' %(iteration))\n","            print('\\n %s \\n' % (text,))   \n","            print('-'*115)\n","\n","            \n","    #increment the pointer and iteration\n","    pointer += seq_length\n","    iteration += 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n"," After 5000 iterations\n","\n","  tree  \n","and i fatte it bean  \n","out e'vemy,cruss i'll arous diys a's can's jukt sifan to su-d!if urees  \n","  \n","how on't or thitt'rca \n","\n","yon  \n","it's bisthour laxt a aline sonellin' s no't a beacs  \n","widbe leanges ars  \n"," to gera mying tt the peawoon, it ig be cat be turs  \n","you meve i ffill taids  \n"," in laymay ma knought se laken tine \"sen't  \n","nut my fomrty ball code soby a lank could pike orking!s  \n","our ille tarul wer, lare ba chay don'g greroag saike!..  \n","the handel caice hancat: a wo beer your lopcan\n","\n"," o \n","\n","-------------------------------------------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-02dd10a6058e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m#train the network and get the final hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     hprev_val, loss_val, _ = sess.run([hprev, loss, updated_gradients],\n\u001b[0;32m---> 30\u001b[0;31m                                       feed_dict={inputs: input_vector,targets: target_vector,init_state: hprev_val})\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"d8lWmWIxj0uP","colab_type":"text"},"source":["Long Short Term Memory Model, with the same dataset."]},{"cell_type":"code","metadata":{"id":"rMFbazHD98YE","colab_type":"code","colab":{}},"source":["# Mapping chars to ints :\n","raw_text = raw_text[:500000]\n","chars = sorted(list(set(raw_text)))\n","int_chars = {i: c for i, c in enumerate(chars)}\n","chars_int = {i: c for c, i in enumerate(chars)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xa5avzX98j0","colab_type":"code","colab":{}},"source":["\n","# Get number of chars and vocab in our text :\n","n_chars = len(raw_text)\n","n_vocab = len(chars)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNHwU1E498um","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1591802230130,"user_tz":-120,"elapsed":519,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"e3588a53-b44e-4a17-f7c7-0de54d2e1da9"},"source":["\n","print('Total Characters : ' , n_chars) \n","print('Total Vocabulary : ', n_vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Characters :  500000\n","Total Vocabulary :  34\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jy_KKt0D-Bzy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1591802237374,"user_tz":-120,"elapsed":4828,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"565d3ddc-c0c2-4a11-e4f7-7ac53b331b16"},"source":["# process the dataset, asw\n","seq_len = 100\n","data_X = []\n","data_y = []\n","\n","for i in range(0, n_chars - seq_len, 1):\n","    # Input Sequeance(will be used as samples)\n","    seq_in  = raw_text[i:i+seq_len]\n","    # Output sequence (will be used as target)\n","    seq_out = raw_text[i + seq_len]\n","    # Store samples in data_X\n","    data_X.append([chars_int[char] for char in seq_in])\n","    # Store targets in data_y\n","    data_y.append(chars_int[seq_out])\n","n_patterns = len(data_X)\n","print( 'Total Patterns : ', n_patterns)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total Patterns :  499900\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qYhyWGK6-B5W","colab_type":"code","colab":{}},"source":["# Reshape X to be suitable to go into LSTM RNN :\n","X = np.reshape(data_X , (n_patterns, seq_len, 1))\n","# Normalizing input data :\n","X = X/ float(n_vocab)\n","# One hot encode the output targets :\n","y = np_utils.to_categorical(data_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_OQL65j-B_G","colab_type":"code","colab":{}},"source":["LSTM_layer_num = 2 # number of LSTM layers\n","layer_size = [256,256] # number of nodes in each layer\n","model = Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GcCmWb_NzqJp","colab_type":"text"},"source":["We need to use the 1.15 version of tensorflow --> we might need to restart the kernel."]},{"cell_type":"code","metadata":{"id":"pZKXsam7znqT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":991},"executionInfo":{"status":"ok","timestamp":1591799957279,"user_tz":-120,"elapsed":112096,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"de0130b4-cf1a-4f2e-af2a-ea3b892a804a"},"source":["!pip install tensorflow==1.15"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n","\u001b[K     |████████████████████████████████| 412.3MB 39kB/s \n","\u001b[?25hCollecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 31.5MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 45.0MB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.29.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.2.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.10.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (47.1.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=0aa6c7d9554cba6c630e0a71a4fc55d49f15e44fca96f60ef76710cb9cb70de9\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n","  Found existing installation: tensorflow-estimator 2.2.0\n","    Uninstalling tensorflow-estimator-2.2.0:\n","      Successfully uninstalled tensorflow-estimator-2.2.0\n","  Found existing installation: tensorboard 2.2.2\n","    Uninstalling tensorboard-2.2.2:\n","      Successfully uninstalled tensorboard-2.2.2\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow 2.2.0\n","    Uninstalling tensorflow-2.2.0:\n","      Successfully uninstalled tensorflow-2.2.0\n","Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["gast","tensorboard","tensorflow"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"-HBOiRP8-CIf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"status":"ok","timestamp":1591802279548,"user_tz":-120,"elapsed":999,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"0f031481-4d8e-48b9-ed8f-0a05b3b67cf3"},"source":["#\n","import tensorflow.contrib\n","\n","model.add(CuDNNLSTM(layer_size[0], input_shape =(X.shape[1], X.shape[2]), return_sequences = True))\n","for i in range(1,LSTM_layer_num) :\n","    model.add(CuDNNLSTM(layer_size[i], return_sequences=True))\n","model.add(Flatten())\n","model.add(Dense(y.shape[1]))\n","model.add(Activation('softmax'))\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","cu_dnnlstm_1 (CuDNNLSTM)     (None, 100, 256)          265216    \n","_________________________________________________________________\n","cu_dnnlstm_2 (CuDNNLSTM)     (None, 100, 256)          526336    \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 25600)             0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 34)                870434    \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 34)                0         \n","=================================================================\n","Total params: 1,661,986\n","Trainable params: 1,661,986\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JTOXcKOd-CDr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1591802584279,"user_tz":-120,"elapsed":277658,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"228387dd-59f0-463a-c6e3-61502c0a62ba"},"source":["model.fit(X,\n","          y,\n","          epochs = 1,\n","           initial_epoch =0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:431: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:438: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","Epoch 1/1\n","499900/499900 [==============================] - 275s 551us/step - loss: 2.6195\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fa58f2f3908>"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"zuufjqYl-bGe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1591802621625,"user_tz":-120,"elapsed":2380,"user":{"displayName":"IAGO NUS","photoUrl":"","userId":"08030727146660354350"}},"outputId":"7a8b2839-4d8a-451f-fb9a-0682235a15c4"},"source":["# set a random seed :\n","start = np.random.randint(0, len(data_X)-1)\n","pattern = data_X[start]\n","#print('Seed : ')\n","print(\"\\\"\",''.join([int_chars[value] for value in pattern]), \"\\\"\\n\")\n","\n","# How many characters you want to generate\n","generated_characters = 400\n","\n","# Generate Charachters :\n","for i in range(generated_characters):\n","    x = np.reshape(pattern, ( 1, len(pattern), 1))\n","    x = x / float(n_vocab)\n","    prediction = model.predict(x,verbose = 0)\n","    index = np.argmax(prediction)\n","    result = int_chars[index]\n","    #seq_in = [int_chars[value] for value in pattern]\n","    sys.stdout.write(result)\n","    pattern.append(index)\n","    pattern = pattern[1:len(pattern)]\n","print('\\nDone')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\" er\n"," dime que me cres\n","dime que me cres\n","dime que sientes cuando me ves\n","cuando me voy cuando no estoy\n","d \"\n","\n","e mue te es la darar\n","co nue de eas a la da lara da ma aada da la aaaaa                                                                                                                                                                                                                                                                                                                                          \n","Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yf89nUVF-bKr","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}